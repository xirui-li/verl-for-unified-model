{"uuid": "ea3ac1a4-fb01-46a6-9736-f7eef3ea61c8", "provider": "UNKNOWN", "num_cpu": 96, "cpu_type": "AMD EPYC 9474F 48-Core Processor", "cpu_family_model_stepping": "25,17,1", "total_memory": 1081885999104, "architecture": "x86_64", "platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "cuda_runtime": "12.4", "gpu_count": 1, "gpu_type": "NVIDIA RTX 6000 Ada Generation", "gpu_memory_per_device": 50893553664, "env_var_json": "{\"VLLM_USE_MODELSCOPE\": false, \"VLLM_USE_TRITON_FLASH_ATTN\": true, \"VLLM_ATTENTION_BACKEND\": null, \"VLLM_USE_FLASHINFER_SAMPLER\": null, \"VLLM_PP_LAYER_PARTITION\": null, \"VLLM_USE_TRITON_AWQ\": false, \"VLLM_USE_V1\": true, \"VLLM_ENABLE_V1_MULTIPROCESSING\": false}", "model_architecture": "Qwen2_5_VLForConditionalGeneration", "vllm_version": "0.8.5", "context": "ENGINE_CONTEXT", "log_time": 1753725076394222080, "source": "production", "dtype": "torch.bfloat16", "tensor_parallel_size": 2, "block_size": 16, "gpu_memory_utilization": 0.6, "quantization": null, "kv_cache_dtype": "auto", "enable_lora": false, "enable_prompt_adapter": false, "enable_prefix_caching": true, "enforce_eager": false, "disable_custom_all_reduce": true}
{"uuid": "2e911dba-770e-4792-ae81-645d45b0f871", "provider": "UNKNOWN", "num_cpu": 96, "cpu_type": "AMD EPYC 9474F 48-Core Processor", "cpu_family_model_stepping": "25,17,1", "total_memory": 1081885999104, "architecture": "x86_64", "platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "cuda_runtime": "12.4", "gpu_count": 1, "gpu_type": "NVIDIA RTX 6000 Ada Generation", "gpu_memory_per_device": 50893553664, "env_var_json": "{\"VLLM_USE_MODELSCOPE\": false, \"VLLM_USE_TRITON_FLASH_ATTN\": true, \"VLLM_ATTENTION_BACKEND\": null, \"VLLM_USE_FLASHINFER_SAMPLER\": null, \"VLLM_PP_LAYER_PARTITION\": null, \"VLLM_USE_TRITON_AWQ\": false, \"VLLM_USE_V1\": true, \"VLLM_ENABLE_V1_MULTIPROCESSING\": false}", "model_architecture": "Qwen2_5_VLForConditionalGeneration", "vllm_version": "0.8.5", "context": "ENGINE_CONTEXT", "log_time": 1753725601868870144, "source": "production", "dtype": "torch.bfloat16", "tensor_parallel_size": 2, "block_size": 16, "gpu_memory_utilization": 0.6, "quantization": null, "kv_cache_dtype": "auto", "enable_lora": false, "enable_prompt_adapter": false, "enable_prefix_caching": true, "enforce_eager": false, "disable_custom_all_reduce": true}
{"uuid": "c37fb678-8dec-4190-8098-58cc11d6a35e", "provider": "UNKNOWN", "num_cpu": 96, "cpu_type": "AMD EPYC 9474F 48-Core Processor", "cpu_family_model_stepping": "25,17,1", "total_memory": 1081885999104, "architecture": "x86_64", "platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "cuda_runtime": "12.4", "gpu_count": 1, "gpu_type": "NVIDIA RTX 6000 Ada Generation", "gpu_memory_per_device": 50893553664, "env_var_json": "{\"VLLM_USE_MODELSCOPE\": false, \"VLLM_USE_TRITON_FLASH_ATTN\": true, \"VLLM_ATTENTION_BACKEND\": null, \"VLLM_USE_FLASHINFER_SAMPLER\": null, \"VLLM_PP_LAYER_PARTITION\": null, \"VLLM_USE_TRITON_AWQ\": false, \"VLLM_USE_V1\": true, \"VLLM_ENABLE_V1_MULTIPROCESSING\": false}", "model_architecture": "Qwen2_5_VLForConditionalGeneration", "vllm_version": "0.8.5", "context": "ENGINE_CONTEXT", "log_time": 1753725851017370880, "source": "production", "dtype": "torch.bfloat16", "tensor_parallel_size": 2, "block_size": 16, "gpu_memory_utilization": 0.6, "quantization": null, "kv_cache_dtype": "auto", "enable_lora": false, "enable_prompt_adapter": false, "enable_prefix_caching": true, "enforce_eager": false, "disable_custom_all_reduce": true}
{"uuid": "0840e364-462b-4ec5-9e0b-466031715cf1", "provider": "UNKNOWN", "num_cpu": 96, "cpu_type": "AMD EPYC 9474F 48-Core Processor", "cpu_family_model_stepping": "25,17,1", "total_memory": 1081885999104, "architecture": "x86_64", "platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "cuda_runtime": "12.4", "gpu_count": 1, "gpu_type": "NVIDIA RTX 6000 Ada Generation", "gpu_memory_per_device": 50893553664, "env_var_json": "{\"VLLM_USE_MODELSCOPE\": false, \"VLLM_USE_TRITON_FLASH_ATTN\": true, \"VLLM_ATTENTION_BACKEND\": null, \"VLLM_USE_FLASHINFER_SAMPLER\": null, \"VLLM_PP_LAYER_PARTITION\": null, \"VLLM_USE_TRITON_AWQ\": false, \"VLLM_USE_V1\": true, \"VLLM_ENABLE_V1_MULTIPROCESSING\": false}", "model_architecture": "Qwen2_5_VLForConditionalGeneration", "vllm_version": "0.8.5", "context": "ENGINE_CONTEXT", "log_time": 1753726185605988096, "source": "production", "dtype": "torch.bfloat16", "tensor_parallel_size": 2, "block_size": 16, "gpu_memory_utilization": 0.6, "quantization": null, "kv_cache_dtype": "auto", "enable_lora": false, "enable_prompt_adapter": false, "enable_prefix_caching": true, "enforce_eager": false, "disable_custom_all_reduce": true}
{"uuid": "d9b5f2d9-6b40-43c4-82d7-8984e692893a", "provider": "UNKNOWN", "num_cpu": 96, "cpu_type": "AMD EPYC 9474F 48-Core Processor", "cpu_family_model_stepping": "25,17,1", "total_memory": 1081885999104, "architecture": "x86_64", "platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "cuda_runtime": "12.4", "gpu_count": 1, "gpu_type": "NVIDIA RTX 6000 Ada Generation", "gpu_memory_per_device": 50893553664, "env_var_json": "{\"VLLM_USE_MODELSCOPE\": false, \"VLLM_USE_TRITON_FLASH_ATTN\": true, \"VLLM_ATTENTION_BACKEND\": null, \"VLLM_USE_FLASHINFER_SAMPLER\": null, \"VLLM_PP_LAYER_PARTITION\": null, \"VLLM_USE_TRITON_AWQ\": false, \"VLLM_USE_V1\": true, \"VLLM_ENABLE_V1_MULTIPROCESSING\": false}", "model_architecture": "Qwen2_5_VLForConditionalGeneration", "vllm_version": "0.8.5", "context": "ENGINE_CONTEXT", "log_time": 1753766800714211072, "source": "production", "dtype": "torch.bfloat16", "tensor_parallel_size": 2, "block_size": 16, "gpu_memory_utilization": 0.6, "quantization": null, "kv_cache_dtype": "auto", "enable_lora": false, "enable_prompt_adapter": false, "enable_prefix_caching": true, "enforce_eager": false, "disable_custom_all_reduce": true}
{"uuid": "b335877e-9aa3-44b2-b130-900801b1b27a", "provider": "UNKNOWN", "num_cpu": 96, "cpu_type": "AMD EPYC 9474F 48-Core Processor", "cpu_family_model_stepping": "25,17,1", "total_memory": 1081885999104, "architecture": "x86_64", "platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "cuda_runtime": "12.4", "gpu_count": 1, "gpu_type": "NVIDIA RTX 6000 Ada Generation", "gpu_memory_per_device": 50893553664, "env_var_json": "{\"VLLM_USE_MODELSCOPE\": false, \"VLLM_USE_TRITON_FLASH_ATTN\": true, \"VLLM_ATTENTION_BACKEND\": null, \"VLLM_USE_FLASHINFER_SAMPLER\": null, \"VLLM_PP_LAYER_PARTITION\": null, \"VLLM_USE_TRITON_AWQ\": false, \"VLLM_USE_V1\": true, \"VLLM_ENABLE_V1_MULTIPROCESSING\": false}", "model_architecture": "Qwen2_5_VLForConditionalGeneration", "vllm_version": "0.8.5", "context": "ENGINE_CONTEXT", "log_time": 1753767555694972928, "source": "production", "dtype": "torch.bfloat16", "tensor_parallel_size": 2, "block_size": 16, "gpu_memory_utilization": 0.6, "quantization": null, "kv_cache_dtype": "auto", "enable_lora": false, "enable_prompt_adapter": false, "enable_prefix_caching": true, "enforce_eager": false, "disable_custom_all_reduce": true}
